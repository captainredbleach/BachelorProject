\chapter{Problem Analysis}

% Introduction
    % Lyngsoe (what do they do?)
    % Ctrl+C, Ctrl+V the paper

% Problem analysis ---
\section{Camera} \label{sec:Camera}
\subsection{Basics of cameras}
\subsubsection{Light}
\subsubsection{Image acquisition}

\subsection{Camera properties}
\subsubsection{Zoom}
\subsubsection{Focus}
\subsubsection{Field of View}
\subsubsection{Aperture}
The principle behind aperture is to limit the visibility of the lens. By doing so, the angles of which the light can enter the camera is vastly reduced. This increases the depth of field, as the image shifts further from a perspective view, and more towards an orthographic view. It is in practice not possible for the camera to transition entirely, but the closer it gets, the further the depth of field expands, along with the focal length narrowing. The major trade-off for this is the sharp reduction in the light entering the camera. This can be compensated for with a longer shutter speed.
\subsubsection{Shutter speed}
The shutter speed of a camera is determined by how long the shutters stay open for the camera. By adjusting the shutter speed, you can manipulate how much light enters the camera, and hits the sensors, thereby altering the brightness of the image. This can be useful when working in a dark environment, or working with very bright elements, such as the sun. The downside of working with extended exposure times is motion blur\citep{PLShutter}. Whenever something is moving, The extended exposure time will make it leave a trail in it's path. This can be desired for artistic reasons, however for image processing, having this blurred trail can be a critical flaw for the application, and as a consequence \glsxtrshort{iso} may be a preferred solution to adjust brightness3\citep{AdobeShutter}.
\subsubsection{ISO}
The most common application of \glsxtrfull{iso} is to brighten or darken pictures, when adjusting the exposure time isn't a viable option. This could be in case of photographing moving elements, or recording video. Adjusting the \glsxtrshort{iso} can alter the image, by Extrapolating imperfections in the captured image, by showing a grainy noise when it is configured high, and by washing out a certain level of fine material texture when configured low\citep{NikonISO}. \glsxtrshort{iso} manipulates the image by altering the data as the sensor inputs are mapped to the image. By working with the raw input, it can produce a much more precise and clear brightening of an image, as compared to what can often be achieved in post processing. \glsxtrshort{iso} Got its name when it's predecessors "ASA" and "DIN" were combined into a single global standard in 1974, adopting the name of the organization that had managed the merging \citep{PLISO}.

\subsection{Data properties}
\subsubsection{Bayer pattern}
\subsubsection{Contrast}
\subsubsection{Color?}\todo{Where color be?}
\section{Image Processing} \label{sec:ImageProcessingGeneral}
% husk introduction!
In this section, the basics of image and image manipulation are described. 

\subsection{Data interpretation and manipulation}
% assuming conversion from camera to digital image has been explained: 
% (By using the image sensor of a camera, each cell from the image sensor is converted into a pixel)
Digital images are represented as two-dimensional arrays. An image is in these cases seen as a discrete function $f(x,y)$. A specific cell or dot in the image is called a pixel. Pixels present the values of the interpreted intensity from cells of the image sensor as described in section \ref{sec:Camera}. Regarding the amount of bits for a single pixel, it is variable depending on the image type and color. Generally, a pixel contains 8 bits (equal to 1 byte) if it is in black-and-white. This means that there is $2^{8} = 256$ different values. For a black-and-white image, the pixels are represented as a gray-scale value ranging from the interval from black (0) to white (255) \citep{book:Moeslund}. \\

When processing an image, it is usually beneficial to be able to modify the key features of an image. Generally, there exist two types of image processing: Point processing and neighborhood processing. For point processing, during an image processing each pixel of the input image is processed and then mapped to the output image with its exact position. This is especially useful for changing brightness and contrast. 

\subsubsection*{Histograms}
An important requirement for image processing is the computer being able to 'see' the image. While a person can see if an image is black-and-white or if the image is too bright or too dark, the case for a computer detecting, for instance, the brightness of the image, is by using the available pixels and count the pixels of different values. This is possible using a histogram, which is described as a discrete function given in equation \ref{eq:HistogramGeneral}:
\begin{equation} \label{eq:HistogramGeneral}
    p(k) = \frac{n_k}{n}
\end{equation}

In equation \ref{eq:HistogramGeneral}, it has been assumed that the image is black-and-white and therefore using a gray-scale intensity. $n$ is the total amount of pixels. $n_k$ refers to the amount of pixel with a gray-scale value of $k$. By dividing $n_k$ with $n$, the probability of a specific gray value of $k$ appearing on the image, $p(k)$, can be found. For color images, the principle is the same, however, three histograms are needed (due to having three channels, red, green and blue). \\

The histogram is generally shown as a graph, where the gray-scale intensity is shown in x-axis and the amount of pixel having that specific intensity is shown on the y-axis as shown in figure \ref{fig:HistogramEx}. Based on the shape of the histogram, the image features such as brightness or contrast can be detected. \\

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{8Misc/Pictures/Introduction/histogram_example.jpg}
	\caption{An example of a histogram. Image from \citep[ Chapter~4.3]{book:Moeslund}}
	\label{fig:HistogramEx}
\end{figure}

While it is possible using a histogram to detect several features of an image, it should be noticed that it is not possible to reconstruct an image based from a histogram. Thereby, it is also possible for several images to have the same histogram \citep{book:Moeslund}. \\

Histograms have so far been described as a tool to recognize distinct features of an image, but they can also be processed such that it can be used as an image manipulation tool - a term known as histogram processing. There exist different types of histogram processing regarding the type of operation needed. In the next section, some types of histogram processing have been described: \\

\textbf{Histogram stretching} \\
If the contrast is too low, the image might appear gray and unclear for humans. The characteristic of an image with low contrast is a narrow difference of gray-level intensity which is shown in the corresponding histogram of the image. An example of a gray-scale image with low contrast is shown in figure \ref{fig:HistogramLowContrast}. 

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{8Misc/Pictures/Introduction/img_lc_full.jpg}
	\caption{Low contrast image with corresponding histogram}
	\label{fig:HistogramLowContrast}
\end{figure}

A solution for this problem is to map the gray-level values into an other value such that the level of gray-level intensities are spread out, which improves the contrast. This method is known as histogram stretching. Based on the original image shown in figure \ref{fig:HistogramLowContrast}, histogram stretching has been used to improve the contrast as shown in figure \ref{fig:HistogramImprovedContrast}.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{8Misc/Pictures/Introduction/img_hc_full.jpg}
	\caption{Improved contrast image from figure \ref{fig:HistogramLowContrast} with corresponding histogram}
	\label{fig:HistogramImprovedContrast}
\end{figure}

\subsubsection*{Thresholding}
An other distinct feature for image processing is the ability to segment the image into several parts such as foreground and background. While segmentation of an image is rather image analysis instead of image manipulation, it contains necessary features for future image manipulation. One method of segmenting an image into several parts is thresholding.

% WIP

\subsubsection*{File format}
Finally, an other important factor in image processing is the size of the image file. A raw digital image contains all data from the image sensor, which means that this type of image files have high quality in regards to details, but will naturally also take up significant memory and storage. This image file is advantageous for editing/manipulating photos, when a very high quality photo is desired. However, for a real-time system, e.g. a surveillance system like a security camera, the bigger file sizes and longer processing times is significantly disadvantageous. For systems, where a smaller file size or faster processing times are needed, it is preferable to compress the image. There exists two types of compression: Lossless and lossy compression. \\

In lossless compression, the data is compressed such that the original data can be be perfectly reconstructed from the compressed data. Thereby, despite that the file size has been reduced due to the compression, no data has been lost. The compression has been done using several algorithms such as Huffman coding \citep{Lossless49:online}. In comparison to lossless compression, lossy compression focuses on minimizing the file size by removing redundant information. Therefore, some data might get lost in the process, which mean that the original image cannot be reconstructed perfectly based on the lossy compressed data. However, the compression algorithm has been optimized from the human vision's point of view, such that the compressed image looks very similar to the original. Compared to lossless compression, lossy compression is very efficient regarding the file size, where the compressed image's file size can be as small as 10\% of the original size without visible degradation of the image quality \citep{LossyDat32:online}. In table \ref{tab:ProConsFileFormat}, the different compression types are compared and their respective advantages and disadvantages have been highlighted. The disadvantages are taken from the point of view of image processing. Here, the preferable outcome is to have a small file size (thereby an efficient compressed image) that do not have any redundant data in the image. 

\begin{table}[H]
\centering
\begin{tabular}{ | p{2.85cm} | p{5.1cm} | p{3.3cm} | p{2.0cm} | }
\hline
\rowcolor{gray!25}
\textbf{Compression type}       & \textbf{Advantage}                                  & \textbf{Disadvantage}              & \textbf{Example of file type}         \\ \hline
No compression                  & Image of highest quality                      &  Big file size            &  .pgm, .ciff                          \\ 
(Raw)                           &                                               &  Long processing time     &                                       \\ \hline
Lossless                        & High quality image with smaller file size     &  Unnecessary data         &  .png, .gif                           \\ \hline
Lossy                           & Decent quality image with significantly smaller file size     & Heavy compression might degrade the image quailty  & .jpeg  \\
                                & Minimal-to-none redundant data                &                           &                                       \\ \hline                                              
\end{tabular}%
\caption{The advantage and disadvantage of different compression types}
\label{tab:ProConsFileFormat}
\end{table}


\subsection{Edge detection}
Edge detection is the process of finding the boundary of an object, meaning that in almost all cases, edges will represent the object in the image. The edge of an object can then be used to get a higher level of abstraction of the image, which means that the information in the image will be less thus easier to process. Before it is possible to find the edge of an image it is important to define what it is. Using a gray-scale image as a baseline it is possible to define an edge as a position where a significant change in gray-level takes place. Looking at figure \ref{fig: edge_pixel} on the left there is shown an image. To find an edge on that image a slice of that image can be taken vertically that is one pixel wide, which is illustrated in the figure between the arrows. By using this slice, a graph can be made where we can interpret the intensity value in the slice as the height in the graph that is shown on the right of figure \ref{fig: edge_pixel}. When looking at the graph when there is a significant change in the gray-scale value in the slice, the graph will also have a significant change in the height. These significant changes in the height is illustrated as circles in the graph, and is where the edges is defined in an image \citep[ Chapter~5.2.2]{book:Moeslund}.

\begin{figure}[H]  %(alternativt [H])
	\centering
	\includegraphics[width=1\textwidth]{8Misc/Pictures/Introduction/edge_pixel.png}
	\caption{A single column of the image is enlarged and presented in a graph. This graph contains
two very significant changes in height, the position of which is marked with circles on the graph.
This is how edges are defined in an image. Image from \citep[ Chapter~5.2.2]{book:Moeslund}}
	\label{fig: edge_pixel}
\end{figure}

To detect edges in an image the directional change in the intensity can be used, this concept is gradient in image processing. This can be done by representing the previous image in a 3D graph as seen in figure \ref{fig: edge_3d}. In the graph it is shown that the width and height of the image is represented by the x- and y-direction respectively, whereas the z-direction is represented by the intensity of the image interpreting it as height. 

\begin{figure}[H]  %(alternativt [H])
	\centering
	\includegraphics[width=1\textwidth]{8Misc/Pictures/Introduction/edge_3d.png}
	\caption{A 3D representation of the image from figure \ref{fig: edge_pixel}, where the intensity of each pixel is interpreted as a height. Image from \citep[ Chapter~5.2.2]{book:Moeslund}}
	\label{fig: edge_3d}
\end{figure}

For each point in the image there would be two partial gradients that would span a plane in the x- and y-directions intersecting a chosen point. These partial gradients can be used to determine the direction of the edge, as well as using their derivatives to calculate the resulting gradient by using eq. \ref{eq:Gradient}

\begin{equation} \label{eq:Gradient}
    \overrightarrow{G}(g_x , g_y)=\begin{pmatrix}
    g_x\\[\jot]
    g_y
    \end{pmatrix}=\begin{pmatrix}
    \frac{\partial f}{\partial x}\\[\jot]
    \frac{\partial f}{\partial y}
    \end{pmatrix}
\end{equation}

A gradient also has a magnitude, which is how steep the change of height is in the direction of the gradient and is also the length of the gradient vector. The length of the gradient vector can be calculated using the Pythagorean theorem solved for magnitude as seen in eq. \ref{eq:magnitude}
\begin{equation} \label{eq:magnitude}
    Magnitude^2 = g_x^2 + g_y^2 \Leftrightarrow Magnitude = \sqrt{g_x^2 + g_y^2}
\end{equation}
For faster implementation the magnitude can be approximated by using eq. \ref{eq:apmag}
\begin{equation} \label{eq:apmag}
    Magnitude = \abs{g_x} + \abs{g_y}
\end{equation}

As the partial gradients are first order derivatives it can not be calculated because an image is not a continuous curve and therefore needs an approximation. To calculate the approximation of the gradient it is possible to use the difference between the previous and next value seen in eq. \ref{eq:apgrad}

\begin{subequations}
    \label{eq:apgrad}
    \begin{align}
    g_x(x, y) \approx f(x+1, y) - f(x-1, y) \label{eq:xapgrad} \\
    g_y(x, y) \approx f(x, y+1) - f(x, y-1) \label{eq:yapgrad}
    \end{align}
\end{subequations}

This approximation will result in a gradient value that is positive when the pixel change from dark to bright and negative when it is reverse. It is possible to get the opposite value by switching the signs seen in eq. \ref{eq:opapgrad}
\begin{subequations}
    \label{eq:opapgrad}
    \begin{align}
    g_x(x, y) \approx f(x-1, y) - f(x+1, y) \label{eq:xopapgrad} \\
    g_y(x, y) \approx f(x, y-1) - f(x, y+1) \label{eq:yopapgrad}
    \end{align}
\end{subequations}

Equation \ref{eq:apgrad} can be applied with a 2D kernel using correlation. A popular 2D kernel that is used is the Sobel kernel where the center pixel is weighed more which can be seen in figure \ref{fig: sobel}

\begin{figure}[H]  %(alternativt [H])
	\centering
	\includegraphics[width=0.4\textwidth]{8Misc/Pictures/Introduction/Sobel.png}
	\caption{Sobel kernel \citep[ Chapter~5.2.2]{book:Moeslund}}
	\label{fig: sobel}
\end{figure}

Looking at the kernel it is seen that the sum of the coefficients gives 0, which means 

%sobel

%canny





\section{Environment}
Det her er en test til at se om at github kan se dette

\section{RFID}

\section{Cages}

\section{Noise}

\section{Scenario}


%%%%% DISPOSITION %%%%%

    % Camera  - Winter
        % Basics of Cameras
            % Light
                % Electromagnetic waves
            % Image acquisition

        % Camera properties
            % Focus
            % FoV
                % Concave vs convex lenses
            % Aperture 
            % Shutterspeed
            % Zoom
            % ISO
            
        % Data properties
            % Bayer pattern
            % Contrast
                % Quantization
                
            % Color
                % Magenta isn't real
                % RGB to HSI< conversion
                % HSI, HSV

    % Image Processing
        % Introduction 
            % Digital image structure
            % Digital color theory

        % Data interpretation and manipulation - Khadar (until thresholding)
            % File format
            % Pixel representation
            % Histogram (relevant??)
            % Thresholding
                % Otsu method
            %Correlation
                % Filters
                    % Median/mean/rank
                    % pattern recognition
            % Morphology
                %Structure element
                % zero/Null
                % Hit (Dilation)
                % Fit (Erosion)
                % Multiple operations
                    % Opening
                    % Closing
            % Blob Detection
                % BLOB!
                % Connected component analysis
                    % Grassfire algorithm
                        % 4 and 8 connectivity
                        %Iterative and recursive
                            % Burn Queue
                    % Region growing
                    % Feature extraction
                    % Feature matching
            
        % Edge detection (steps + methods) - Kazim
            % Sobel edge detection
            % Canny edge detection
            
        % Computer Vision and Machine learning
            % OpenCV
            % TensorFlow
            
    
            
    
    % Environment

    % RFID
        % Antenna
        % Tags
        % 96 bit
        % Aktiv
            % Exciter scenario
        % Passiv
        % Phasing

    % Cage
        % Dimensions
        % Shape

    % Background Noise
        % Light pollution
        % Radio pollution
            % Excess objects
                % Humans
                % Trucklift
                % Machines
                % Packages
    
    % Scenarios
        % Objects
        % Use cases
        % Use case diagram
            
                
        

% Problem specification (what is the problem) (different chapter)
    % Usage of cameras (how many cameras do we need to solve the problem?)

    % System specification 
        % Accepttests


% Unsorted things
    % Tile image to determine movement (possible solution)
    % Angle of incidence